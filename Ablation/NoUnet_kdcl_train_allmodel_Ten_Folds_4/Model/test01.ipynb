{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "input shape:  torch.Size([1, 4, 625])\n",
      "output shape:  torch.Size([1, 1, 625])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 16, 313]             192\n",
      "       BatchNorm1d-2              [-1, 16, 313]              32\n",
      "         Hardswish-3              [-1, 16, 313]               0\n",
      "            Conv1d-4              [-1, 16, 157]              48\n",
      "       BatchNorm1d-5              [-1, 16, 157]              32\n",
      "              ReLU-6              [-1, 16, 157]               0\n",
      "            Conv1d-7                 [-1, 8, 1]             136\n",
      "            Conv1d-8                [-1, 16, 1]             144\n",
      " SqueezeExcitation-9              [-1, 16, 157]               0\n",
      "           Conv1d-10              [-1, 16, 157]             256\n",
      "      BatchNorm1d-11              [-1, 16, 157]              32\n",
      "         Identity-12              [-1, 16, 157]               0\n",
      " InvertedResidual-13              [-1, 16, 157]               0\n",
      "           Conv1d-14              [-1, 72, 157]           1,152\n",
      "      BatchNorm1d-15              [-1, 72, 157]             144\n",
      "             ReLU-16              [-1, 72, 157]               0\n",
      "           Conv1d-17               [-1, 72, 79]             216\n",
      "      BatchNorm1d-18               [-1, 72, 79]             144\n",
      "             ReLU-19               [-1, 72, 79]               0\n",
      "           Conv1d-20               [-1, 24, 79]           1,728\n",
      "      BatchNorm1d-21               [-1, 24, 79]              48\n",
      "         Identity-22               [-1, 24, 79]               0\n",
      " InvertedResidual-23               [-1, 24, 79]               0\n",
      "           Conv1d-24               [-1, 88, 79]           2,112\n",
      "      BatchNorm1d-25               [-1, 88, 79]             176\n",
      "             ReLU-26               [-1, 88, 79]               0\n",
      "           Conv1d-27               [-1, 88, 79]             264\n",
      "      BatchNorm1d-28               [-1, 88, 79]             176\n",
      "             ReLU-29               [-1, 88, 79]               0\n",
      "           Conv1d-30               [-1, 24, 79]           2,112\n",
      "      BatchNorm1d-31               [-1, 24, 79]              48\n",
      "         Identity-32               [-1, 24, 79]               0\n",
      " InvertedResidual-33               [-1, 24, 79]               0\n",
      "           Conv1d-34               [-1, 96, 79]           2,304\n",
      "      BatchNorm1d-35               [-1, 96, 79]             192\n",
      "        Hardswish-36               [-1, 96, 79]               0\n",
      "           Conv1d-37               [-1, 96, 40]             480\n",
      "      BatchNorm1d-38               [-1, 96, 40]             192\n",
      "        Hardswish-39               [-1, 96, 40]               0\n",
      "           Conv1d-40                [-1, 24, 1]           2,328\n",
      "           Conv1d-41                [-1, 96, 1]           2,400\n",
      "SqueezeExcitation-42               [-1, 96, 40]               0\n",
      "           Conv1d-43               [-1, 40, 40]           3,840\n",
      "      BatchNorm1d-44               [-1, 40, 40]              80\n",
      "         Identity-45               [-1, 40, 40]               0\n",
      " InvertedResidual-46               [-1, 40, 40]               0\n",
      "           Conv1d-47              [-1, 240, 40]           9,600\n",
      "      BatchNorm1d-48              [-1, 240, 40]             480\n",
      "        Hardswish-49              [-1, 240, 40]               0\n",
      "           Conv1d-50              [-1, 240, 40]           1,200\n",
      "      BatchNorm1d-51              [-1, 240, 40]             480\n",
      "        Hardswish-52              [-1, 240, 40]               0\n",
      "           Conv1d-53                [-1, 64, 1]          15,424\n",
      "           Conv1d-54               [-1, 240, 1]          15,600\n",
      "SqueezeExcitation-55              [-1, 240, 40]               0\n",
      "           Conv1d-56               [-1, 40, 40]           9,600\n",
      "      BatchNorm1d-57               [-1, 40, 40]              80\n",
      "         Identity-58               [-1, 40, 40]               0\n",
      " InvertedResidual-59               [-1, 40, 40]               0\n",
      "           Conv1d-60              [-1, 240, 40]           9,600\n",
      "      BatchNorm1d-61              [-1, 240, 40]             480\n",
      "        Hardswish-62              [-1, 240, 40]               0\n",
      "           Conv1d-63              [-1, 240, 40]           1,200\n",
      "      BatchNorm1d-64              [-1, 240, 40]             480\n",
      "        Hardswish-65              [-1, 240, 40]               0\n",
      "           Conv1d-66                [-1, 64, 1]          15,424\n",
      "           Conv1d-67               [-1, 240, 1]          15,600\n",
      "SqueezeExcitation-68              [-1, 240, 40]               0\n",
      "           Conv1d-69               [-1, 40, 40]           9,600\n",
      "      BatchNorm1d-70               [-1, 40, 40]              80\n",
      "         Identity-71               [-1, 40, 40]               0\n",
      " InvertedResidual-72               [-1, 40, 40]               0\n",
      "           Conv1d-73              [-1, 120, 40]           4,800\n",
      "      BatchNorm1d-74              [-1, 120, 40]             240\n",
      "        Hardswish-75              [-1, 120, 40]               0\n",
      "           Conv1d-76              [-1, 120, 40]             600\n",
      "      BatchNorm1d-77              [-1, 120, 40]             240\n",
      "        Hardswish-78              [-1, 120, 40]               0\n",
      "           Conv1d-79                [-1, 32, 1]           3,872\n",
      "           Conv1d-80               [-1, 120, 1]           3,960\n",
      "SqueezeExcitation-81              [-1, 120, 40]               0\n",
      "           Conv1d-82               [-1, 48, 40]           5,760\n",
      "      BatchNorm1d-83               [-1, 48, 40]              96\n",
      "         Identity-84               [-1, 48, 40]               0\n",
      " InvertedResidual-85               [-1, 48, 40]               0\n",
      "           Conv1d-86              [-1, 144, 40]           6,912\n",
      "      BatchNorm1d-87              [-1, 144, 40]             288\n",
      "        Hardswish-88              [-1, 144, 40]               0\n",
      "           Conv1d-89              [-1, 144, 40]             720\n",
      "      BatchNorm1d-90              [-1, 144, 40]             288\n",
      "        Hardswish-91              [-1, 144, 40]               0\n",
      "           Conv1d-92                [-1, 40, 1]           5,800\n",
      "           Conv1d-93               [-1, 144, 1]           5,904\n",
      "SqueezeExcitation-94              [-1, 144, 40]               0\n",
      "           Conv1d-95               [-1, 48, 40]           6,912\n",
      "      BatchNorm1d-96               [-1, 48, 40]              96\n",
      "         Identity-97               [-1, 48, 40]               0\n",
      " InvertedResidual-98               [-1, 48, 40]               0\n",
      "           Conv1d-99              [-1, 288, 40]          13,824\n",
      "     BatchNorm1d-100              [-1, 288, 40]             576\n",
      "       Hardswish-101              [-1, 288, 40]               0\n",
      "          Conv1d-102              [-1, 288, 20]           1,440\n",
      "     BatchNorm1d-103              [-1, 288, 20]             576\n",
      "       Hardswish-104              [-1, 288, 20]               0\n",
      "          Conv1d-105                [-1, 72, 1]          20,808\n",
      "          Conv1d-106               [-1, 288, 1]          21,024\n",
      "SqueezeExcitation-107              [-1, 288, 20]               0\n",
      "          Conv1d-108               [-1, 96, 20]          27,648\n",
      "     BatchNorm1d-109               [-1, 96, 20]             192\n",
      "        Identity-110               [-1, 96, 20]               0\n",
      "InvertedResidual-111               [-1, 96, 20]               0\n",
      "          Conv1d-112              [-1, 576, 20]          55,296\n",
      "     BatchNorm1d-113              [-1, 576, 20]           1,152\n",
      "       Hardswish-114              [-1, 576, 20]               0\n",
      "          Conv1d-115              [-1, 576, 10]           2,880\n",
      "     BatchNorm1d-116              [-1, 576, 10]           1,152\n",
      "       Hardswish-117              [-1, 576, 10]               0\n",
      "          Conv1d-118               [-1, 144, 1]          83,088\n",
      "          Conv1d-119               [-1, 576, 1]          83,520\n",
      "SqueezeExcitation-120              [-1, 576, 10]               0\n",
      "          Conv1d-121               [-1, 96, 10]          55,296\n",
      "     BatchNorm1d-122               [-1, 96, 10]             192\n",
      "        Identity-123               [-1, 96, 10]               0\n",
      "InvertedResidual-124               [-1, 96, 10]               0\n",
      "          Conv1d-125              [-1, 576, 10]          55,296\n",
      "     BatchNorm1d-126              [-1, 576, 10]           1,152\n",
      "       Hardswish-127              [-1, 576, 10]               0\n",
      "          Conv1d-128               [-1, 576, 5]           2,880\n",
      "     BatchNorm1d-129               [-1, 576, 5]           1,152\n",
      "       Hardswish-130               [-1, 576, 5]               0\n",
      "          Conv1d-131               [-1, 144, 1]          83,088\n",
      "          Conv1d-132               [-1, 576, 1]          83,520\n",
      "SqueezeExcitation-133               [-1, 576, 5]               0\n",
      "          Conv1d-134                [-1, 96, 5]          55,296\n",
      "     BatchNorm1d-135                [-1, 96, 5]             192\n",
      "        Identity-136                [-1, 96, 5]               0\n",
      "InvertedResidual-137                [-1, 96, 5]               0\n",
      "          Conv1d-138               [-1, 576, 5]          55,296\n",
      "     BatchNorm1d-139               [-1, 576, 5]           1,152\n",
      "       Hardswish-140               [-1, 576, 5]               0\n",
      "AdaptiveAvgPool1d-141               [-1, 576, 1]               0\n",
      "          Linear-142                 [-1, 1024]         590,848\n",
      "       Hardswish-143                 [-1, 1024]               0\n",
      "         Dropout-144                 [-1, 1024]               0\n",
      "          Linear-145                  [-1, 625]         640,625\n",
      "================================================================\n",
      "Total params: 2,111,585\n",
      "Trainable params: 2,111,585\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.56\n",
      "Params size (MB): 8.06\n",
      "Estimated Total Size (MB): 12.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from functools import partial\n",
    "from typing import Callable, List, Optional\n",
    "# from pytorch_model_summary import summary\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "def _make_divisible(ch, divisor=8, min_ch=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_ch is None:\n",
    "        min_ch = divisor\n",
    "    new_ch = max(min_ch, int(ch + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_ch < 0.9 * ch:\n",
    "        new_ch += divisor\n",
    "    return new_ch\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, input_c: int, squeeze_factor: int = 4):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        squeeze_c = _make_divisible(input_c // squeeze_factor, 8)\n",
    "        self.fc1 = nn.Conv1d(input_c, squeeze_c, 1)\n",
    "        self.fc2 = nn.Conv1d(squeeze_c, input_c, 1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        scale = F.adaptive_avg_pool1d(x, output_size=1)\n",
    "        scale = self.fc1(scale)\n",
    "        scale = F.relu(scale, inplace=True)\n",
    "        scale = self.fc2(scale)\n",
    "        scale = F.hardsigmoid(scale, inplace=True)\n",
    "        return scale * x\n",
    "\n",
    "\n",
    "class ConvBNActivation(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 in_planes: int,\n",
    "                 out_planes: int,\n",
    "                 kernel_size: int = 3,\n",
    "                 stride: int = 1,\n",
    "                 groups: int = 1,\n",
    "                 norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "                 activation_layer: Optional[Callable[..., nn.Module]] = None):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        if activation_layer is None:\n",
    "            activation_layer = nn.ReLU6\n",
    "        super(ConvBNActivation, self).__init__(nn.Conv1d(in_channels=in_planes,\n",
    "                                                         out_channels=out_planes,\n",
    "                                                         kernel_size=kernel_size,\n",
    "                                                         stride=stride,\n",
    "                                                         padding=padding,\n",
    "                                                         groups=groups,\n",
    "                                                         bias=False),\n",
    "                                               norm_layer(out_planes),\n",
    "                                               activation_layer(inplace=True))\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, exp_size, stride, use_se, use_hs,\n",
    "                 norm_layer: Callable[..., nn.Module]):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "\n",
    "        if stride not in [1, 2]:\n",
    "            raise ValueError(\"illegal stride value.\")\n",
    "\n",
    "        self.use_res_connect = (stride == 1 and in_planes == out_planes)\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        activation_layer = nn.Hardswish if use_hs else nn.ReLU\n",
    "\n",
    "        # expand\n",
    "        if exp_size != in_planes:\n",
    "            layers.append(ConvBNActivation(in_planes,\n",
    "                                           exp_size,\n",
    "                                           kernel_size=1,\n",
    "                                           norm_layer=norm_layer,\n",
    "                                           activation_layer=activation_layer))\n",
    "\n",
    "        # depthwise\n",
    "        layers.append(ConvBNActivation(exp_size,\n",
    "                                       exp_size,\n",
    "                                       kernel_size=kernel_size,\n",
    "                                       stride=stride,\n",
    "                                       groups=exp_size,\n",
    "                                       norm_layer=norm_layer,\n",
    "                                       activation_layer=activation_layer))\n",
    "\n",
    "        if use_se:\n",
    "            layers.append(SqueezeExcitation(exp_size))\n",
    "\n",
    "        # project\n",
    "        layers.append(ConvBNActivation(exp_size,\n",
    "                                       out_planes,\n",
    "                                       kernel_size=1,\n",
    "                                       norm_layer=norm_layer,\n",
    "                                       activation_layer=nn.Identity))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        self.out_channels = out_planes\n",
    "        self.is_strided = stride > 1\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        result = self.block(x)\n",
    "        if self.use_res_connect:\n",
    "            result += x\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class modile_net_v3_small(nn.Module):\n",
    "    def __init__(self, out_planes):\n",
    "        super(modile_net_v3_small, self).__init__()\n",
    "        in_planes = 4\n",
    "        norm_layer = partial(nn.BatchNorm1d, eps=0.001, momentum=0.01)\n",
    "        # 第一层\n",
    "        self.in_1 = nn.Sequential(ConvBNActivation(in_planes,\n",
    "                                                   16,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=2,\n",
    "                                                   norm_layer=norm_layer,\n",
    "                                                   activation_layer=nn.Hardswish)\n",
    "                                  )\n",
    "        # 第二层\n",
    "        self.in_2 = InvertedResidual(in_planes=16, out_planes=16, kernel_size=3,\n",
    "                                     exp_size=16, stride=2, use_se=1, use_hs=0, norm_layer=norm_layer)\n",
    "        self.in_3 = InvertedResidual(in_planes=16, out_planes=24, kernel_size=3,\n",
    "                                     exp_size=72, stride=2, use_se=0, use_hs=0, norm_layer=norm_layer)\n",
    "        self.in_4 = InvertedResidual(in_planes=24, out_planes=24, kernel_size=3,\n",
    "                                     exp_size=88, stride=1, use_se=0, use_hs=0, norm_layer=norm_layer)\n",
    "        self.in_5 = InvertedResidual(in_planes=24, out_planes=40, kernel_size=5,\n",
    "                                     exp_size=96, stride=2, use_se=1, use_hs=1, norm_layer=norm_layer)\n",
    "        self.in_6 = InvertedResidual(in_planes=40, out_planes=40, kernel_size=5,\n",
    "                                     exp_size=240, stride=1, use_se=1, use_hs=1, norm_layer=norm_layer)\n",
    "        self.in_7 = InvertedResidual(in_planes=40, out_planes=40, kernel_size=5,\n",
    "                                     exp_size=240, stride=1, use_se=1, use_hs=1, norm_layer=norm_layer)\n",
    "        self.in_8 = InvertedResidual(in_planes=40, out_planes=48, kernel_size=5,\n",
    "                                     exp_size=120, stride=1, use_se=1, use_hs=1, norm_layer=norm_layer)\n",
    "        self.in_9 = InvertedResidual(in_planes=48, out_planes=48, kernel_size=5,\n",
    "                                     exp_size=144, stride=1, use_se=1, use_hs=1, norm_layer=norm_layer)\n",
    "        self.in_10 = InvertedResidual(in_planes=48, out_planes=96, kernel_size=5,\n",
    "                                      exp_size=288, stride=2, use_se=1, use_hs=1, norm_layer=norm_layer)\n",
    "        self.in_11 = InvertedResidual(in_planes=96, out_planes=96, kernel_size=5,\n",
    "                                      exp_size=576, stride=2, use_se=1, use_hs=1, norm_layer=norm_layer)\n",
    "        self.in_12 = InvertedResidual(in_planes=96, out_planes=96, kernel_size=5,\n",
    "                                      exp_size=576, stride=2, use_se=1, use_hs=1, norm_layer=norm_layer)\n",
    "        self.in_13 = ConvBNActivation(96,\n",
    "                                      576,\n",
    "                                      kernel_size=1,\n",
    "                                      norm_layer=norm_layer,\n",
    "                                      activation_layer=nn.Hardswish)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        last_channel = _make_divisible(1280)\n",
    "        print(last_channel)\n",
    "        last_channel = 1024\n",
    "        self.classifier = nn.Sequential(nn.Linear(576, last_channel),\n",
    "                                        nn.Hardswish(inplace=True),\n",
    "                                        nn.Dropout(p=0.2, inplace=True),\n",
    "                                        nn.Linear(last_channel, out_planes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.in_1(x)\n",
    "        x2 = self.in_2(x1)\n",
    "        x3 = self.in_3(x2)\n",
    "        x4 = self.in_4(x3)\n",
    "        x5 = self.in_5(x4)\n",
    "        x6 = self.in_6(x5)\n",
    "        x7 = self.in_7(x6)\n",
    "        x8 = self.in_8(x7)\n",
    "        x9 = self.in_9(x8)\n",
    "        x10 = self.in_10(x9)\n",
    "        x11 = self.in_11(x10)\n",
    "        x12 = self.in_12(x11)\n",
    "        x13 = self.in_13(x12)\n",
    "        x14 = self.avgpool(x13)\n",
    "        x14 = torch.flatten(x14, 1)\n",
    "        x15 = self.classifier(x14)\n",
    "        x15 = x15.unsqueeze(dim=1)\n",
    "        return x15\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a = torch.randn(1, 4, 625)\n",
    "    model = modile_net_v3_small(625)\n",
    "    print(\"input shape: \", a.shape)\n",
    "    print(\"output shape: \", model(a).shape)\n",
    "    summary(model, (4,625), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
